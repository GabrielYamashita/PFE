{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL9EB0Mn9ny5"
   },
   "source": [
    "# P2P - Aula 07 - Simulação com Widgets\n",
    "\n",
    "### Raul Ikeda - rauligs@insper.edu.br\n",
    "\n",
    "---\n",
    "\n",
    "Trabalhando com simulação usando widgets. \n",
    "\n",
    "O material abaixo foi adaptado da documentação oficial: https://pypi.org/project/pyportfolioopt/\n",
    "\n",
    "---\n",
    "\n",
    "# Mean-variance optimization\n",
    "\n",
    "In this cookbook recipe, we work on several examples demonstrating PyPortfolioOpt's mean-variance capabilities. I will discuss what I think should be your \"default\" options, based on my experience in optimising portfolios.\n",
    "\n",
    "To start, you need a list of tickers. Some people just provide the whole universe of stocks, but I don't think this is a good idea - portfolio optimization is quite different from asset selection. I would suggest anywhere from 10-50 stocks as a starting point.\n",
    "\n",
    "Some of the things we cover:\n",
    "\n",
    "- Downloading data and getting it into PyPortfolioOpt\n",
    "- Calculating and visualising the covariance matrix\n",
    "- Optimising a long/short portfolio to minimise total variance\n",
    "- Optimising a portfolio to maximise the Sharpe ratio, subject to sector constraints\n",
    "- Optimising a portfolio to maximise return for a given risk, subject to sector constraints, with an L2 regularisation objective\n",
    "- Optimising a market-neutral portfolio to minimise risk for a given level of return\n",
    "- Optimising along the mean-semivariance frontier\n",
    "- Optimising along the mean-CVaR frontier\n",
    "- Plotting the efficient frontier:\n",
    "    - Simple (using CLA)\n",
    "    - Constrained\n",
    "    - Complex plots\n",
    "\n",
    "Please consult the [docs](https://pyportfolioopt.readthedocs.io/) for more info.\n",
    "\n",
    "## Downloading data\n",
    "\n",
    "To download data, we will use `yfinance`, an excellent library that provides free price data from Yahoo Finance, no API key needed.\n",
    "\n",
    "**Nota**: Já instalamos a biblioteca em encontros anteriores. Rodar o comando novamente não afeta a instalação, mas é bom deixar caso algum aluno tenha trocado o computador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69vaYVwZ-Cxz",
    "outputId": "dee1bf4a-82ee-4909-e26b-217899258cf2"
   },
   "outputs": [],
   "source": [
    "!pip install yfinance PyPortfolioOpt ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shuJGGeo9ny8"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4r8LJdC9ny8"
   },
   "outputs": [],
   "source": [
    "# Microsoft, Amazon, Coca-cola, Mastercard, Costo\n",
    "# Southwest Airlines, ExxonMobil, Pfizer, JPMorgan Chase, UnitedHealth Group\n",
    "# Accenture, Disney, Gilead, Ford, Tesla\n",
    "tickers = [\"MSFT\", \"AMZN\", \"KO\", \"MA\", \"COST\", \n",
    "           \"LUV\", \"XOM\", \"PFE\", \"JPM\", \"UNH\", \n",
    "           \"ACN\", \"DIS\", \"GILD\", \"F\", \"TSLA\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWdfXpB69ny9",
    "outputId": "e60980bf-21e3-470f-a4f7-99da82c820a4"
   },
   "outputs": [],
   "source": [
    "ohlc = yf.download(tickers, period=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "S7voZG_T9ny-",
    "outputId": "de26041a-af6d-4554-eb32-1eb9d57d5f3e"
   },
   "outputs": [],
   "source": [
    "prices = ohlc[\"Adj Close\"].dropna(how=\"all\")\n",
    "prices.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "-8Et4dlp9ny-",
    "outputId": "e35cbcf6-f837-4a84-9946-b7a31d4b6901"
   },
   "outputs": [],
   "source": [
    "prices[prices.index >= \"2008-01-01\"].plot(figsize=(15,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4vdGWLR9ny-"
   },
   "source": [
    "## Calculating the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dTFkI4ll9ny-",
    "outputId": "2a0c7a48-d357-4445-bdd1-44dbe901c17a"
   },
   "outputs": [],
   "source": [
    "import pypfopt\n",
    "pypfopt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "0hDN8ZJG9ny_",
    "outputId": "35244fed-56ac-4207-ee84-736e924b1210"
   },
   "outputs": [],
   "source": [
    "from pypfopt import risk_models\n",
    "from pypfopt import plotting\n",
    "\n",
    "# Calculate annualised Covariance Matrix\n",
    "sample_cov = risk_models.sample_cov(prices, frequency=252)\n",
    "sample_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "HZ15mDtz9ny_",
    "outputId": "5fe64424-4756-4289-e29e-39edc3eb83df"
   },
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "plotting.plot_covariance(sample_cov, plot_correlation=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bhjN57j9ny_"
   },
   "source": [
    "For reasons discussed in the docs, the sample covariance matrix should not be your default choice. I think a better option is Ledoit-Wolf shrinkage, which reduces the extreme values in the covariance matrix. In the image below, we can see that there are fewer bright spots outside the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "htpILjX-9nzA",
    "outputId": "254cc6d7-bbee-48bf-c888-82c85a96bf83"
   },
   "outputs": [],
   "source": [
    "S = risk_models.CovarianceShrinkage(prices).ledoit_wolf()\n",
    "plotting.plot_covariance(S, plot_correlation=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mbpNBp39nzA"
   },
   "source": [
    "## Return estimation\n",
    "\n",
    "As discussed in the docs, it is often a bad idea to provide returns using a simple estimate like the mean of past returns. Unless you have a proprietary method for estimating returns, research suggests that you may be better off not providing expected returns – you can then just find the `min_volatility()` portfolio or use `HRP`. \n",
    "\n",
    "However, in this example we will use the CAPM returns, which aims to be slightly more stable than the default mean historical return. Please see the notebook `1-RiskReturnModels.ipynb` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_syhOUu9nzH",
    "outputId": "40e40fd5-06ac-4f42-c375-7a240c15c27e"
   },
   "outputs": [],
   "source": [
    "from pypfopt import expected_returns\n",
    "\n",
    "mu = expected_returns.capm_return(prices)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "MlqP3XP99nzH",
    "outputId": "f5e026c2-d259-4894-d3dc-a667d591e101"
   },
   "outputs": [],
   "source": [
    "mu.plot.barh(figsize=(10,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHNXRsJA9nzI"
   },
   "source": [
    "## Long/short min variance\n",
    "\n",
    "In this section, we construct a long/short portfolio with the objective of minimising variance. There is a good deal of research that demonstrates that these global-minimum variance (GMV) portfolios outperform mean-variance optimized portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTjcmcmK9nzI"
   },
   "outputs": [],
   "source": [
    "from pypfopt import EfficientFrontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD5_4rlY9nzI",
    "outputId": "81faef92-6667-4452-8859-3a659d235b14"
   },
   "outputs": [],
   "source": [
    "S = risk_models.CovarianceShrinkage(prices).ledoit_wolf()\n",
    "\n",
    "# You don't have to provide expected returns in this case\n",
    "ef = EfficientFrontier(None, S, weight_bounds=(None, None))\n",
    "ef.min_volatility()\n",
    "weights = ef.clean_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "RVRwHED99nzJ",
    "outputId": "f606418a-1614-4c38-9574-c18b1b9c5ac6"
   },
   "outputs": [],
   "source": [
    "pd.Series(weights).plot.barh();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpgqSknF9nzJ"
   },
   "source": [
    "We can get a quick indication of the portfolio performance as follows. Note that this is an in sample estimate and may have very little resemblance to how the portfolio actually performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnDm8UkC9nzJ",
    "outputId": "9302f8ed-698e-461b-c4a1-ad8113f1aa11"
   },
   "outputs": [],
   "source": [
    "ef.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "aYa6yhWx9nzJ",
    "outputId": "0d5cf83b-05f5-44b4-92ae-8f2ab51f6a96"
   },
   "outputs": [],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORCrBIG19nzK"
   },
   "source": [
    "Let's say we were happy with this portfolio and wanted to actually go out and buy the shares.  To do this, we would need to construct a **discrete allocation** (unless your broker supports fractional shares!)\n",
    "\n",
    "If we had \\$20,0000 to invest and would like our portfolio to be 130/30 long/short, we can construct the actual allocation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w10Rcb-X9nzK",
    "outputId": "052edfa3-7811-459c-debb-644dc7aa2368"
   },
   "outputs": [],
   "source": [
    "from pypfopt import DiscreteAllocation\n",
    "\n",
    "latest_prices = prices.iloc[-1]  # prices as of the day you are allocating\n",
    "da = DiscreteAllocation(weights, latest_prices, total_portfolio_value=20000, short_ratio=0.3)\n",
    "alloc, leftover = da.lp_portfolio()\n",
    "print(f\"Discrete allocation performed with ${leftover:.2f} leftover\")\n",
    "alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab3Rk-Aa9nzK"
   },
   "source": [
    "## Max Sharpe with sector constraints\n",
    "\n",
    "If you have your own model for returns (or have read the warnings and want to proceed anyways), you may consider maximising the Sharpe ratio. This theoretically gives the optimal portfolio in terms of risks-returns.\n",
    "\n",
    "In this section, we construct a long-only max-sharpe portfolio, but also incorporate sector constraints. Sector constraints require three things. A `sector_mapper`, your `sector_lower` bounds, and your `sector_upper` bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJcyi2-E9nzK"
   },
   "outputs": [],
   "source": [
    "sector_mapper = {\n",
    "    \"MSFT\": \"Tech\",\n",
    "    \"AMZN\": \"Consumer Discretionary\",\n",
    "    \"KO\": \"Consumer Staples\",\n",
    "    \"MA\": \"Financial Services\",\n",
    "    \"COST\": \"Consumer Staples\",\n",
    "    \"LUV\": \"Aerospace\",\n",
    "    \"XOM\": \"Energy\",\n",
    "    \"PFE\": \"Healthcare\",\n",
    "    \"JPM\": \"Financial Services\",\n",
    "    \"UNH\": \"Healthcare\",\n",
    "    \"ACN\": \"Misc\",\n",
    "    \"DIS\": \"Media\",\n",
    "    \"GILD\": \"Healthcare\",\n",
    "    \"F\": \"Auto\",\n",
    "    \"TSLA\": \"Auto\"\n",
    "}\n",
    "\n",
    "sector_lower = {\n",
    "    \"Consumer Staples\": 0.1, # at least 10% to staples\n",
    "    \"Tech\": 0.05 # at least 5% to tech\n",
    "    # For all other sectors, it will be assumed there is no lower bound\n",
    "}\n",
    "\n",
    "sector_upper = {\n",
    "    \"Tech\": 0.2,\n",
    "    \"Aerospace\":0.1,\n",
    "    \"Energy\": 0.1,\n",
    "    \"Auto\":0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h5ZdZwF9nzL"
   },
   "source": [
    "We then set up the optimizer and add our constraints. We can use `ef.add_objective()` to add other constraints. For example, let's say that in addition to the above sector constraints, I specifically want:\n",
    "\n",
    "- 10% of the portfolio in AMZN\n",
    "- Less than 5% of my portfolio in TSLA\n",
    "- At least 5% of my portfolio in MSFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRjwf0wN9nzL"
   },
   "outputs": [],
   "source": [
    "mu = expected_returns.capm_return(prices)\n",
    "S = risk_models.CovarianceShrinkage(prices).ledoit_wolf()\n",
    "\n",
    "ef = EfficientFrontier(mu, S)  # weight_bounds automatically set to (0, 1)\n",
    "ef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\n",
    "\n",
    "amzn_index = ef.tickers.index(\"AMZN\")\n",
    "ef.add_constraint(lambda w: w[amzn_index] == 0.10)\n",
    "\n",
    "tsla_index = ef.tickers.index(\"TSLA\")\n",
    "ef.add_constraint(lambda w: w[tsla_index] <= 0.05)\n",
    "\n",
    "msft_index = ef.tickers.index(\"MSFT\")\n",
    "ef.add_constraint(lambda w: w[msft_index] >= 0.05)\n",
    "\n",
    "ef.max_sharpe()\n",
    "weights = ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZAu8ypG9nzL",
    "outputId": "2a3703a8-bbd0-468d-f0c6-25be50f7b0c7"
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "izbZQ6-G9nzL",
    "outputId": "1aafb3d3-a32b-469e-e702-6044c7f54116"
   },
   "outputs": [],
   "source": [
    "pd.Series(weights).plot.pie();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGa9xMQ19nzM"
   },
   "source": [
    "We can immediately see that our explicit constraints were satisfied, and can check all the sector constraints as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtA8KzWp9nzM",
    "outputId": "cd007325-fa24-4f41-f8fa-3d149ae7fc23"
   },
   "outputs": [],
   "source": [
    "# O(N^2) loop not a good idea in a coding interview :)\n",
    "for sector in set(sector_mapper.values()):\n",
    "    total_weight = 0\n",
    "    for t,w in weights.items():\n",
    "        if sector_mapper[t] == sector:\n",
    "            total_weight += w\n",
    "    print(f\"{sector}: {total_weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "fig = plt.Figure(figsize=(10,10))\n",
    "\n",
    "slider = FloatSlider(\n",
    "    value=0.20,\n",
    "    min=0.15,\n",
    "    max=0.40,\n",
    "    step=0.05,\n",
    "    description='Risk:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "def update(risk = 0.15):\n",
    "    ef = EfficientFrontier(mu, S)\n",
    "    ef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\n",
    "    ef.efficient_risk(risk)\n",
    "    weights = ef.clean_weights()\n",
    "    pd.Series(weights).plot.pie();    \n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update, risk = slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOZ-z4AK9nzM"
   },
   "source": [
    "## Maximise return for a given risk, with L2 regularisation\n",
    "\n",
    "Let's imagine that we've put a lot of thought into our risk tolerance, and have decided that we can't accept anything more than 15% volatility. We can use PyPortfolioOpt to construct a portfolio that maximises return for a given risk (with the same caveats about expected returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKantkrg9nzM",
    "outputId": "3b60679f-80ef-4ec7-e029-989ef9d27f14"
   },
   "outputs": [],
   "source": [
    "ef = EfficientFrontier(mu, S)\n",
    "ef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\n",
    "ef.efficient_risk(target_volatility=0.15)\n",
    "weights = ef.clean_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46ylv28y9nzM",
    "outputId": "de8f0548-437e-4997-cc46-d927d5f61df2"
   },
   "outputs": [],
   "source": [
    "num_small = len([k for k in weights if weights[k] <= 1e-4])\n",
    "print(f\"{num_small}/{len(ef.tickers)} tickers have zero weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HWpBJxZ9nzM",
    "outputId": "cae10c97-978e-40ee-8dd5-796d9635f53f"
   },
   "outputs": [],
   "source": [
    "ef.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFdT8Xar9nzN"
   },
   "source": [
    "While this portfolio seems like it meets our objectives, we might be worried by the fact that a lot of the tickers have been assigned zero weight. In effect, the optimizer is \"overfitting\" to the data you have provided -- you are much more likely to get better results by enforcing some level of diversification. One way of doing this is to use **L2 regularisation** – essentially, adding a penalty on the number of near-zero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiaa_L2u9nzN",
    "outputId": "0adbf717-faaa-4142-9fe3-6ca9368cc23f"
   },
   "outputs": [],
   "source": [
    "from pypfopt import objective_functions\n",
    "\n",
    "# You must always create a new efficient frontier object\n",
    "ef = EfficientFrontier(mu, S)\n",
    "ef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\n",
    "ef.add_objective(objective_functions.L2_reg, gamma=0.1)  # gamma is the tuning parameter\n",
    "ef.efficient_risk(0.15)\n",
    "weights = ef.clean_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0ZO5l4D9nzN",
    "outputId": "aefa431d-90b2-493b-8232-8dc8c8213212"
   },
   "outputs": [],
   "source": [
    "num_small = len([k for k in weights if weights[k] <= 1e-4])\n",
    "print(f\"{num_small}/{len(ef.tickers)} tickers have zero weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8yo6ey19nzN"
   },
   "source": [
    "We can tune the value of gamma to choose the number of nonzero tickers. Larger gamma pulls portfolio weights towards an equal allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIdHgQnE9nzO",
    "outputId": "76b74cec-dbad-4278-ad5c-6a3bb99b404d"
   },
   "outputs": [],
   "source": [
    "ef = EfficientFrontier(mu, S)\n",
    "ef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\n",
    "ef.add_objective(objective_functions.L2_reg, gamma=1)  # gamma is the tuning parameter\n",
    "ef.efficient_risk(0.15)\n",
    "weights = ef.clean_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "p0DzMHim9nzO",
    "outputId": "baaaab0c-4bee-4883-c0fb-70b4d650ef90"
   },
   "outputs": [],
   "source": [
    "pd.Series(weights).plot.pie(figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFLNfMLr9nzO",
    "outputId": "69c1cead-6233-465a-a3c3-6c5def23d624"
   },
   "outputs": [],
   "source": [
    "ef.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wggcvvSR9nzO"
   },
   "source": [
    "The resulting portfolio still has a volatility of less than our 15% limit. It's in-sample Sharpe ratio has gone down, but this portfolio is a lot more robust for actual investment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1r-Z6ws9nzO"
   },
   "source": [
    "## Minimise risk for a given return, market-neutral\n",
    "\n",
    "We may instead be in the situation where we have a certain required rate of return (maybe we are a pension fund that needs 7% return a year), but would like to minimise risk. Additionally, suppose we would like our portfolio to be market neutral, in the sense that it is equally exposed to the long and short sides.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XB4Cw8at9nzO",
    "outputId": "26404432-1324-435e-cde0-c28a242a571e"
   },
   "outputs": [],
   "source": [
    "# Must have no weight bounds to allow shorts\n",
    "ef = EfficientFrontier(mu, S, weight_bounds=(None, None))\n",
    "ef.add_objective(objective_functions.L2_reg)\n",
    "ef.efficient_return(target_return=0.07, market_neutral=True)\n",
    "weights = ef.clean_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uf3oUKFP9nzO",
    "outputId": "aa726cf0-1200-4fa9-f61d-ca5807679a34"
   },
   "outputs": [],
   "source": [
    "ef.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "4rrQKk1Q9nzP",
    "outputId": "95a52da6-37ed-4dad-cd5b-2701affa6695"
   },
   "outputs": [],
   "source": [
    "pd.Series(weights).plot.barh(figsize=(10,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwR1iKqv9nzP",
    "outputId": "478e0b8b-c1ed-4826-a1bc-1ad928927123"
   },
   "outputs": [],
   "source": [
    "print(f\"Net weight: {sum(weights.values()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBvVeqtm9nzP"
   },
   "source": [
    "## Efficient semi-variance optimization\n",
    "\n",
    "In this example, we will minimise the portfolio semivariance (i.e downside volatility) subject to a return constraint (target 20%).\n",
    "\n",
    "There are actually two ways of doing this in PyPortfolioOpt. The first is the \"intuitive\" way. We compute a semicovariance matrix, and pass this into `EfficientFrontier` (just like we would do for the exponential cov matrix or the Ledoit-Wolf shrunk matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8FsS5Yqu9nzP",
    "outputId": "94948fd4-598a-4fa7-a41a-371fbda43f43"
   },
   "outputs": [],
   "source": [
    "semicov = risk_models.semicovariance(prices, benchmark=0)\n",
    "plotting.plot_covariance(semicov);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cK0NfwQ69nzP",
    "outputId": "16b22337-b5ac-42cc-fe4c-8543d96275fd"
   },
   "outputs": [],
   "source": [
    "ef = EfficientFrontier(mu, semicov)\n",
    "ef.efficient_return(0.2)\n",
    "weights = ef.clean_weights()\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUmSYq629nzP",
    "outputId": "62eba498-9e70-41b9-c361-5ad7f7814348"
   },
   "outputs": [],
   "source": [
    "ef.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZZeruY69nzP"
   },
   "source": [
    "However, this solution is not truly optimal in mean-semivariance space. To do the optimization properly, we must use the `EfficientSemivariance` class. This requires us to first compute the returns and drop NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qGD5WW69nzQ"
   },
   "outputs": [],
   "source": [
    "returns = expected_returns.returns_from_prices(prices)\n",
    "returns = returns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tyeys0NY9nzQ",
    "outputId": "bf3aa95f-99ab-4410-e9f9-c04e10bf82eb"
   },
   "outputs": [],
   "source": [
    "from pypfopt import EfficientSemivariance\n",
    "\n",
    "es = EfficientSemivariance(mu, returns)\n",
    "es.efficient_return(0.2)\n",
    "es.portfolio_performance(verbose=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g-7u4YA9nzQ"
   },
   "source": [
    "To compare this with the heuristic solution, I will use a quick hack: replacing the `es.weights` with `es.weights` and running `es.portfolio_performance` again. Please don't be encouraged to do this in real life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jLzfOCj9nzQ",
    "outputId": "ba8f4fcc-882a-4764-b0b8-e3906677dbd7"
   },
   "outputs": [],
   "source": [
    "es.weights = ef.weights\n",
    "es.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FK4C6ah89nzQ"
   },
   "source": [
    "We see that the heuristic method has a significantly lower Sortino ratio, and much higher semivariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJAcvz899nzQ"
   },
   "source": [
    "## Efficient CVaR optimization\n",
    "\n",
    "In this example, we will find the portfolio that maximises return subject to a CVaR constraint.\n",
    "\n",
    "Before doing this, let's first compute the 95%-CVaR for the max-sharpe portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "x1d3qupJ9nzR",
    "outputId": "daadeac7-706a-445e-fdd6-ab02efc6d005"
   },
   "outputs": [],
   "source": [
    "returns = expected_returns.returns_from_prices(prices).dropna()\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RdDFhAt9nzR",
    "outputId": "76d8a6e4-c2b9-4c39-e525-e7b8cf2444e7"
   },
   "outputs": [],
   "source": [
    "ef = EfficientFrontier(mu, S)\n",
    "ef.max_sharpe()\n",
    "weight_arr = ef.weights\n",
    "ef.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "29a0FtJY9nzR",
    "outputId": "a053e96e-e6fc-4c93-92ce-2cd3373ae112"
   },
   "outputs": [],
   "source": [
    "# Compute CVaR\n",
    "portfolio_rets = (returns * weight_arr).sum(axis=1)\n",
    "portfolio_rets.hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LLBZ15l9nzR",
    "outputId": "5f81ae25-bf09-495e-c389-9df5890717d0"
   },
   "outputs": [],
   "source": [
    "# VaR\n",
    "var = portfolio_rets.quantile(0.05)\n",
    "cvar = portfolio_rets[portfolio_rets <= var].mean()\n",
    "print(\"VaR: {:.2f}%\".format(100*var))\n",
    "print(\"CVaR: {:.2f}%\".format(100*cvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxHWPjSz9nzR"
   },
   "source": [
    "This value of the CVaR means that our average loss on the worst 5% of days will be -3.35%. Let's say that this were beyond our comfort zone (for a \\\\$100,000 portfolio, this would mean losing \\\\$3350 in a day).\n",
    "\n",
    "Let's firstly construct the portfolio with the minimum CVaR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQ79Y0Gr9nzR",
    "outputId": "b92a041c-9e59-45ba-8cab-ac427d889126"
   },
   "outputs": [],
   "source": [
    "from pypfopt import EfficientCVaR\n",
    "\n",
    "ec = EfficientCVaR(mu, returns)\n",
    "ec.min_cvar()\n",
    "ec.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVEjdNOv9nzS"
   },
   "source": [
    "We have significantly reduced the CVaR, but at the cost of a large reduction in returns. We can use `efficient_risk` to maximise the return for a target risk. Let's say that a 2.5% CVaR is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onq8FVDr9nzS",
    "outputId": "32b82ef5-e86c-414d-b965-d44e27813ff3"
   },
   "outputs": [],
   "source": [
    "from pypfopt import EfficientCVaR\n",
    "\n",
    "ec = EfficientCVaR(mu, returns)\n",
    "ec.efficient_risk(target_cvar=0.025)\n",
    "ec.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wjVOkqd9nzS"
   },
   "source": [
    "We now have similar returns to before (24.7% vs 25.8%), but with a lower tail risk (2.50% CVaR vs 3.35%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ_DCIo19nzS"
   },
   "source": [
    "## Plotting - Unconstrained\n",
    "\n",
    "To plot the unconstrained efficient frontier, it is easiest to use the critical line algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0UWSKGj9nzS",
    "outputId": "08a6de2a-17e4-4f70-92ea-fe86a9c8f442"
   },
   "outputs": [],
   "source": [
    "from pypfopt import CLA, plotting\n",
    "\n",
    "cla = CLA(mu, S)\n",
    "cla.max_sharpe()\n",
    "cla.portfolio_performance(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Sk2eFFZ79nzS",
    "outputId": "6f967d10-32c4-4f83-ecac-aed15aa8180a"
   },
   "outputs": [],
   "source": [
    "ax = plotting.plot_efficient_frontier(cla, showfig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aURS39QR9nzS"
   },
   "source": [
    "## Plotting - Constrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xPJH-s69nzT"
   },
   "source": [
    "In this example, we will plot the efficient frontier corresponding to portfolios with a constraint on exposure to MSFT, AMZN, and TSLA (e.g maybe we want to avoid big tech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OavRfyqJ9nzT"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "mu = expected_returns.capm_return(prices)\n",
    "S = risk_models.CovarianceShrinkage(prices).ledoit_wolf()\n",
    "\n",
    "ef = EfficientFrontier(mu, S,)\n",
    "\n",
    "big_tech_indices = [t in {\"MSFT\", \"AMZN\", \"TSLA\"} for t in tickers]\n",
    "ef.add_constraint(lambda w: cp.sum(w[big_tech_indices]) <= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPJ5mK8p9nzT"
   },
   "source": [
    "As per the docs, *before* we call any optimization function, we should pass this to the plotting module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotting.plot_efficient_frontier(ef, ef_param=\"return\", \n",
    "                                      ef_param_range=np.linspace(0.15, 0.40, 100), \n",
    "                                      show_tickers = True,\n",
    "                                      showfig=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht1M5kIo9nzT"
   },
   "source": [
    "## Complex plots\n",
    "\n",
    "The `plotting` module allows you to pass in an `ax`, on top of which the plots are added. This allows you to build complex plots. \n",
    "\n",
    "In this example, we will plot the efficient frontier as well as 10,000 simulated portfolios. \n",
    "\n",
    "To generate the simulated portfolios, we will sample random weights from the Dirichlet distribution (these are already normalised):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esHxc73V9nzT",
    "outputId": "decb7120-4d2e-49f3-f7d9-fcb5b78cdcce"
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "w = np.random.dirichlet(np.ones(len(mu)), n_samples)\n",
    "rets = w.dot(mu)\n",
    "stds = np.sqrt((w.T * (S @ w.T)).sum(axis=0))\n",
    "sharpes = rets / stds\n",
    "\n",
    "print(\"Sample portfolio returns:\", rets)\n",
    "print(\"Sample portfolio volatilities:\", stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "WjCjkC9p9nzU",
    "outputId": "386b35b3-75fd-4186-8f65-f16317246965"
   },
   "outputs": [],
   "source": [
    "# Plot efficient frontier with Monte Carlo sim\n",
    "ef = EfficientFrontier(mu, S)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plotting.plot_efficient_frontier(ef, ax=ax, show_assets=False)\n",
    "\n",
    "# Find and plot the tangency portfolio\n",
    "ef2 = EfficientFrontier(mu, S)\n",
    "ef2.max_sharpe()\n",
    "ret_tangent, std_tangent, _ = ef2.portfolio_performance()\n",
    "ax.scatter(std_tangent, ret_tangent, marker=\"*\", s=100, c=\"r\", label=\"Max Sharpe\")\n",
    "\n",
    "# Plot random portfolios\n",
    "ax.scatter(stds, rets, marker=\".\", c=sharpes, cmap=\"viridis_r\")\n",
    "\n",
    "# Format\n",
    "ax.set_title(\"Efficient Frontier with random portfolios\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZ0tR7Lt9nzU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PBvVeqtm9nzP"
   ],
   "name": "2-Mean-Variance-Optimisation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
